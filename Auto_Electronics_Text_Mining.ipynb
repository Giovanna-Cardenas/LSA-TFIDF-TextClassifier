{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d321c4d-93be-4751-9732-76bf314335ff",
   "metadata": {},
   "source": [
    "# Text Classification Using TF-IDF, LSA, and Random Forest\n",
    "**Author:** Giovanna Cardenas  \n",
    "**Description:** This notebook performs binary text classification to distinguish between automotive and electronics product reviews. It applies two preprocessing strategies—one with stemming and one without—to compare their impact on model performance. The classification pipeline uses TF-IDF vectorization, Latent Semantic Analysis (LSA), and a Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "499d37dc-a845-4b05-87f6-256f7f943df5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/giovannacardenas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load Packages\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, ENGLISH_STOP_WORDS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from dmba import printTermDocumentMatrix, classificationSummary\n",
    "nltk.download('punkt')\n",
    "import random\n",
    "random.seed(10)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd8dd3-4b5d-4fe0-894b-598f8bb73622",
   "metadata": {},
   "source": [
    "### Process without Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "de4dd836-3c87-4ce3-9e19-9cd04ef80a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the zipped file and create document corpus and label vector (0 for electronics, 1 for autos).\n",
    "# 'ns' prefix used to differentiate data that will not go through stemming process.\n",
    "ns_corpus = []\n",
    "ns_label = []\n",
    "with ZipFile('AutoAndElectronics.zip') as rawData:\n",
    "    for info in rawData.infolist():\n",
    "        if info.is_dir(): \n",
    "            continue\n",
    "        ns_label.append(1 if 'rec.autos' in info.filename else 0)\n",
    "        ns_corpus.append(rawData.read(info))\n",
    "\n",
    "# Preprocessing (tokenization and stopwords without stemming)\n",
    "class SimpleTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "    def __call__(self, doc):\n",
    "        return [t for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t.lower() not in self.stopWords]\n",
    "\n",
    "# Vectorize the corpus using the custom tokenizer\n",
    "ns_preprocessor = CountVectorizer(tokenizer=SimpleTokenizer(), encoding='latin1')\n",
    "ns_preprocessedText = ns_preprocessor.fit_transform(ns_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "05b1b9da-895a-4892-a0d5-e84427788e81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Sentence 1  Sentence 2  Sentence 3  Sentence 4  Sentence 5  \\\n",
      "subject              2           1           1           2           1   \n",
      "lines                1           1           1           1           1   \n",
      "apr                  1           1           1           1           1   \n",
      "date                 1           1           1           1           1   \n",
      "newsgroups           1           1           1           1           1   \n",
      "\n",
      "            Sentence 6  Sentence 7  Sentence 8  Sentence 9  Sentence 10  ...  \\\n",
      "subject              1           1           1           1            1  ...   \n",
      "lines                1           1           1           1            1  ...   \n",
      "apr                  1           1           1           1            1  ...   \n",
      "date                 1           1           1           1            1  ...   \n",
      "newsgroups           1           1           1           1            1  ...   \n",
      "\n",
      "            Sentence 1991  Sentence 1992  Sentence 1993  Sentence 1994  \\\n",
      "subject                 1              1              1              1   \n",
      "lines                   1              1              1              1   \n",
      "apr                     1              1              1              1   \n",
      "date                    1              1              1              1   \n",
      "newsgroups              1              1              1              1   \n",
      "\n",
      "            Sentence 1995  Sentence 1996  Sentence 1997  Sentence 1998  \\\n",
      "subject                 1              2              1              2   \n",
      "lines                   1              1              1              1   \n",
      "apr                     1              1              1              2   \n",
      "date                    1              1              1              2   \n",
      "newsgroups              1              1              1              1   \n",
      "\n",
      "            Sentence 1999  Sentence 2000  \n",
      "subject                 2              1  \n",
      "lines                   1              1  \n",
      "apr                     1              1  \n",
      "date                    1              1  \n",
      "newsgroups              1              1  \n",
      "\n",
      "[5 rows x 2000 columns]\n"
     ]
    }
   ],
   "source": [
    "# Build a term document matrix\n",
    "ns_td = pd.DataFrame(ns_preprocessedText.todense())\n",
    "ns_td.columns = ns_preprocessor.get_feature_names_out()\n",
    "ns_term_document_matrix = ns_td.T\n",
    "ns_term_document_matrix.columns = ['Sentence '+str(i) for i in range(1, ns_td.shape[0]+1)]\n",
    "ns_term_document_matrix['total_count'] = ns_term_document_matrix.sum(axis=1)\n",
    "\n",
    "#Top 25 most frequent words \n",
    "ns_term_document_matrix = ns_term_document_matrix.sort_values(by ='total_count',ascending=False)[:25] \n",
    "\n",
    "# Print the first 5 rows \n",
    "print(ns_term_document_matrix.drop(columns=['total_count']).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b15bf89a-8d81-418a-a062-82975e000d83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 18721)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of df without stemming\n",
    "ns_preprocessedText.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9bcca2-1fba-47b1-9274-d62b1c97aec0",
   "metadata": {},
   "source": [
    "### Process with Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "43b33255-081c-4589-9c01-3993d1a6f804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reload the zipped data to create a new corpus and label vector for stemming\n",
    "corpus = []\n",
    "label = []\n",
    "with ZipFile('AutoAndElectronics.zip') as rawData:\n",
    "    for info in rawData.infolist():\n",
    "        if info.is_dir(): \n",
    "            continue\n",
    "        label.append(1 if 'rec.autos' in info.filename else 0)\n",
    "        corpus.append(rawData.read(info))\n",
    "\n",
    "# Preprocessing (tokenization, stemming, and stopwords)\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "# Vectorize with stemming\n",
    "preprocessor = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='latin1')\n",
    "preprocessedText = preprocessor.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f54d0e2c-285a-4ad2-96c2-713c3898220d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Sentence 1  Sentence 2  Sentence 3  Sentence 4  Sentence 5  \\\n",
      "line                1           1           1           1           2   \n",
      "subject             2           1           1           2           1   \n",
      "car                12           1           0           0           5   \n",
      "apr                 1           1           1           1           1   \n",
      "newsgroup           1           1           1           1           1   \n",
      "\n",
      "           Sentence 6  Sentence 7  Sentence 8  Sentence 9  Sentence 10  ...  \\\n",
      "line                1           1           1           1            1  ...   \n",
      "subject             1           1           1           1            1  ...   \n",
      "car                 0           2           3           2            3  ...   \n",
      "apr                 1           1           1           1            1  ...   \n",
      "newsgroup           1           1           1           1            1  ...   \n",
      "\n",
      "           Sentence 1991  Sentence 1992  Sentence 1993  Sentence 1994  \\\n",
      "line                   2              1              1              1   \n",
      "subject                1              1              1              1   \n",
      "car                    0              2              0              0   \n",
      "apr                    1              1              1              1   \n",
      "newsgroup              1              1              1              1   \n",
      "\n",
      "           Sentence 1995  Sentence 1996  Sentence 1997  Sentence 1998  \\\n",
      "line                   1              1              1              1   \n",
      "subject                1              2              1              2   \n",
      "car                    0              0              0              0   \n",
      "apr                    1              1              1              2   \n",
      "newsgroup              1              1              1              1   \n",
      "\n",
      "           Sentence 1999  Sentence 2000  \n",
      "line                   1              1  \n",
      "subject                2              1  \n",
      "car                    0              0  \n",
      "apr                    1              1  \n",
      "newsgroup              1              1  \n",
      "\n",
      "[5 rows x 2000 columns]\n"
     ]
    }
   ],
   "source": [
    "# Build a term document matrix\n",
    "td = pd.DataFrame(preprocessedText.todense())\n",
    "td.columns = preprocessor.get_feature_names_out()\n",
    "term_document_matrix = td.T\n",
    "term_document_matrix.columns = ['Sentence '+str(i) for i in range(1, td.shape[0]+1)]\n",
    "term_document_matrix['total_count'] = term_document_matrix.sum(axis=1)\n",
    "\n",
    "#Top 25 most frequent words \n",
    "term_document_matrix = term_document_matrix.sort_values(by ='total_count',ascending=False)[:25] \n",
    "\n",
    "# Print the first 5 rows \n",
    "print(term_document_matrix.drop(columns=['total_count']).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "416643c0-4e6a-4a57-b0cc-874eaf9ec634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 13516)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of df with stemming\n",
    "preprocessedText.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a6b39d-ca86-409a-b7a1-4d14fcff90c5",
   "metadata": {},
   "source": [
    "### TF-IDF + LSA without Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "aba5f473-fc9e-4b4a-a3a9-72a939bee61c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9575)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 393  13\n",
      "     1  21 373\n"
     ]
    }
   ],
   "source": [
    "# Compute TF-IDF\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "tfidf = tfidfTransformer.fit_transform(ns_preprocessedText)\n",
    "\n",
    "# Extract 10 concepts using LSA ()\n",
    "svd = TruncatedSVD(10, random_state=10)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "lsa_tfidf = lsa.fit_transform(tfidf)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size=0.4, random_state=10)\n",
    "\n",
    "# Run Random Forest Classifier model on training\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=5)\n",
    "rf_classifier.fit(Xtrain, ytrain)\n",
    "\n",
    "# Print confusion matrix and accuracy for df without stemming\n",
    "classificationSummary(ytest, rf_classifier.predict(Xtest))\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "tfidf = tfidfTransformer.fit_transform(ns_preprocessedText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d813d9-dc24-4010-bd16-7387b5c8a640",
   "metadata": {},
   "source": [
    "### TF-IDF + LSA with Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d5943937-54fb-46d9-9f28-933ac973d204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9613)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 393  13\n",
      "     1  18 376\n"
     ]
    }
   ],
   "source": [
    "# Compute TF-IDF\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "tfidf = tfidfTransformer.fit_transform(preprocessedText)\n",
    "\n",
    "# Extract 10 concepts using LSA ()\n",
    "svd = TruncatedSVD(10, random_state= 10)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "lsa_tfidf = lsa.fit_transform(tfidf)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size=0.4, random_state=10)\n",
    "\n",
    "# Run Random Forest Classifier model on training\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=5)\n",
    "rf_classifier.fit(Xtrain, ytrain)\n",
    "\n",
    "# Print confusion matrix and accuracy for df with stemming\n",
    "classificationSummary(ytest, rf_classifier.predict(Xtest))\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "tfidf = tfidfTransformer.fit_transform(preprocessedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "59c62eab-0fa7-427d-8ef5-ead1c076fee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stemming reduced our data frame by 5205 rows which helps reduce redundancy. Also, without stemming, the accuracy of the model is 95.75% but with stemming, it is 96.13%.\n",
    "\n",
    "# The concept matrix reports what tokens are present, frequent, or infrequent. The TF-IDF measures the importance of each token."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
